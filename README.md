# baixing_application

### 2. 有用户反映说他不能访问我们网站，而你是系统工程师，你准备分哪几步来找到问题？ 


### 3. 想一想你实现过的一个解决方案，说说它解决了什么问题，有哪些优点、哪些缺点，这样设计的原因是什么，以及对其他人产生了什么样的影响。
目前公司做跨境电商业务，订单信息要经过前台系统（用户下单支付）、后台ERP（审核）、WMS仓储系统、清关事务对接系统、海关系统等众多异构系统。业务人员如果要找一个订单的流转状态，可能要登录不同的系统去查看。  
目前单量也不是很大，提出使用filebeat+logstash+elasticsearch的方案。
  1. 在订单的每一操作点生成统一格式日志写入文件。包括订单号（唯一）、操作人、操作时间、在哪个系统操作、状态变化等。
  2. 在产生日志的服务器上安装filebeat。或将某些服务器日志通过nfs映射到安装有logstash的服务器上。
  3. 三台虚拟机安装elasticsearch做集群。
  4. 开发简单的可视化界面展现订单流转图以及统计。
  5. 通过ldap控制业务人员权限。
  6. 通过elasticsearch的聚合功能结合定时任务实现报警。比如超过3天未出订单、超过30分钟未收到海关接口回执等。

######优点
  1. 快速实现
  2. 对各个系统代码侵入性小
  3. 部署和扩展方便  

######缺点
  1. 代码埋点地方太多
  2. 需要在服务器安装客户端
  3. 写日志会影响性能


### 4. web日志分隔
1. split_log_main.sh为主文件，调用split_log_sub.sh  
  `/bin/bash split_log_main.sh <logfile> <processnum> "Googlebot" "Baiduspider" "^404"`
2. 主要思路  
  1. 先获取log文件第一行和最后一行的日志大小，取平均，然后根据文件大小估算出log文件总行数。使用总行数除以<processnum>得到分隔行数，以此分隔行数使用split将log文件先分隔为近似processnum个小文件；  
  2. 然后调用sub脚本，后台运行，启动脚本进程个数与分隔文件个数相等，脚本中先使用grep过滤，然后使用awk，由于log文件是按照时间排序的，所以只在上下行日期发生变化时计算目标文件日期名称。  
  3. 最后统一对生成的日志进行压缩，多进程后台运行。

